{"name":"Practical Machine Learning Course Project","tagline":"","body":"# Background\r\n\r\nUsing devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible \r\nto collect a large amount of data about personal activity relatively \r\ninexpensively. These type of devices are part of the quantified self movement â€“ \r\na group of enthusiasts who take measurements about themselves regularly to \r\nimprove their health, to find patterns in their behavior, or because they are \r\ntech geeks. One thing that people regularly do is quantify how much of a \r\nparticular activity they do, but they rarely quantify how well they do it. In \r\nthis project, your goal will be to use data from accelerometers on the belt, \r\nforearm, arm, and dumbell of 6 participants. They were asked to perform barbell \r\nlifts correctly and incorrectly in 5 different ways. More information is \r\navailable from the website here: http://groupware.les.inf.puc-rio.br/har (see \r\nthe section on the Weight Lifting Exercise Dataset). \r\n\r\n# Data \r\n\r\n - The training data for this project are available here: \r\n https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\r\n - The test data are available here: \r\n https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\r\n\r\nLet's download them if they are not yet in the working directory and load them \r\ninto memory.\r\n\r\n```{r cache=TRUE}\r\nif(file.exists(\"pml-training.csv\")==FALSE) \r\n    {\r\n    download.file(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv\"\r\n                  ,destfile = \"pml-training.csv\"\r\n                  #,method=\"curl\"\r\n                  )\r\n     }\r\nif(file.exists(\"pml-testing.csv\")==FALSE) \r\n    {\r\n    download.file(\"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv\"\r\n                  ,destfile = \"pml-testing.csv\"\r\n                  #,method=\"curl\"\r\n                  )\r\n     }\r\n# Training dataset goes to variables with 'pml' prefix\r\npml <- read.csv(\"pml-training.csv\")\r\n```\r\n\r\nThe data for this project come from this source: \r\nhttp://groupware.les.inf.puc-rio.br/har.\r\n\r\n# Goal\r\n\r\nThe goal of this project is to predict the manner in which they did the \r\nexercise. This is the `classe` variable in the training set. Any of the other \r\nvariables are allowed use for prediction. This report describes how the model \r\nwas built, how cross validation was used, what the expected out of sample error \r\nis, and why or how the choices were made.\r\n\r\n# Preconditions\r\n\r\nLet's load the necessary packages.\r\n\r\n - `caret` for machnine learning libraries\r\n - `plyr` & `dplyr` for data manipulation\r\n - `doParallel` for exploiting the power of multi core machines\r\n\r\n```{r}\r\nlibrary(caret)\r\nlibrary(plyr)\r\nlibrary(dplyr)\r\nlibrary(doParallel)\r\n```\r\n\r\nI am \"setting seed\" for reproducible results.\r\n\r\n```{r}\r\nset.seed(1234)\r\n```\r\n\r\n# Data splitting\r\n\r\nAs always, my first step is to slice up our given 'testing' dataset to training \r\nand testing datasets that enables us to estimate out of sample error of a \r\nparticular model. Testing dataset will not be touched until my model is ready.\r\nThus, any type of exploratory data analysis will be executed only on training \r\nset to avoid my model to be biased by testing data part. I used a randomly \r\nselected 75% of original training set to train my model.\r\n\r\n```{r}\r\nintrain <- createDataPartition(y = pml$classe, p=0.75, list=FALSE)\r\npmltrain <- pml[intrain,]\r\npmltest <- pml[-intrain,]\r\n```\r\n\r\nLet's take a look at the training dataset and the predictable outcome variable.\r\n\r\n```{r}\r\nstr(pmltrain, list.len = 200)\r\nsummary(pmltrain$classe)\r\n```\r\n\r\n# Data Transformation and Processing\r\n\r\nI decided to remove `user_name` since it can bring bias in the prediction \r\n(overfitting to this specific dataset) but would not add to the prediction power \r\nof a generalized model. (E.g. if Carlitos always performed his exercise \r\ncorrectly then learning algorythms would make use of this information and apply\r\nthis information for prediction. However, it might be useful information for our\r\ngiven test set but not for generalized model, where we do not have the any user \r\ninformation.)\r\n\r\nAlso, removal of timestamp fields (i.e. `raw_timestamp_part_1`, \r\n`raw_timestamp_part_2`, `cvtd_timestamp`) makes sense, since these measures do\r\nnot add any value for a general model. In the time of this analysis meaning of \r\nvariables of `new_window` & `num_window` were unknown for me but they clearly \r\nnot reflect accelerometer measures so I left them out from the further analysis.\r\nVariable of `X` has been ignored upfront as well, since it represents a simple\r\nrecord identifier.\r\n\r\n```{r}\r\npmltrainc <- pmltrain %>% select(-X,\r\n                                 -user_name,\r\n                                 -raw_timestamp_part_1,\r\n                                 -raw_timestamp_part_2,\r\n                                 -cvtd_timestamp,\r\n                                 -new_window,\r\n                                 -num_window,\r\n                                 -classe)\r\n```\r\n\r\nI found that several factor variables were actually containing numeric\r\ninformation. Just to make sure that a simple factor-numeric conversion would not\r\nfail, I checked the levels for all factor variables.\r\n\r\n```{r}\r\nsapply(pmltrainc[,sapply(pmltrainc,is.factor)],levels)\r\n```\r\n\r\nTwo particular values, `#DIV/0!` and empty values bring some data quality issue,\r\nso I handled it by replacing them with `NA`.\r\n\r\n```{r}\r\npmltrainc[pmltrainc==\"#DIV/0!\"] <- NA\r\npmltrainc[pmltrainc==\"\"] <- NA\r\n```\r\n\r\nThen I converted all the factor variables to numeric.\r\n\r\n```{r}\r\n# http://stackoverflow.com/questions/8596466/r-change-all-columns-of-type-factor-to-numeric\r\n# Applying the wisdom from Carl Witthoft above:\r\nasNumeric <- function(x) as.numeric(as.character(x))\r\nfactorsNumeric <- function(d) modifyList(d, lapply(d[, sapply(d, is.factor)],   \r\n                                                   asNumeric))\r\n\r\n# Convert to numeric\r\npmltraincnum <- factorsNumeric(pmltrainc)\r\n```\r\n\r\nTo avoid model building errors, I replaced NAs with zeros.\r\n\r\n```{r}\r\npmltraincnum[is.na(pmltraincnum)] <- 0\r\n```\r\n\r\nEventually I finalized the dataset for further analyses by including `classe`\r\ntarget variable.\r\n\r\n```{r}\r\npmltraincnum <- cbind(classe=pmltrain$classe,pmltraincnum)\r\n```\r\n\r\nThese transformations have to be performed on test dataset as well.\r\n\r\n```{r}\r\npmltestc <- pmltest %>% select(-X,\r\n                               -user_name,\r\n                               -raw_timestamp_part_1,\r\n                               -raw_timestamp_part_2,\r\n                               -cvtd_timestamp,\r\n                               -new_window,\r\n                               -num_window,\r\n                               -classe)\r\npmltestc[pmltestc==\"#DIV/0!\"] <- NA\r\npmltestc[pmltestc==\"\"] <- NA\r\npmltestcnum <- factorsNumeric(pmltestc)\r\npmltestcnum[is.na(pmltestcnum)] <- 0\r\npmltestcnum <- cbind(classe=pmltest$classe,pmltestcnum)\r\n```\r\n\r\n## Dimension Reduction\r\n\r\nIn order to build robust prediction model I needed to reduct the number of\r\nindependent variables. At this stage I still had 152 variables that is too much\r\nfor building a classifier model on top of that which returns results in a\r\nreasonable time frame.\r\n\r\n### Removing Zero Covariates\r\n\r\nThe first and most obvious task to be performed here is to identify and filter\r\nout covariates with zero variance. (Since they will not have any explanation\r\npower on my model for sure.)\r\n\r\n```{r}\r\npmltraincnumnonzv <- pmltraincnum[,nearZeroVar(pmltraincnum,\r\n                                               saveMetrics=TRUE)$zeroVar==FALSE]\r\n```\r\n\r\nThe following 9 variables have been filter out in this step:\r\n\r\n```{r}\r\nrownames(\r\n    nearZeroVar(pmltraincnum,\r\n                saveMetrics=TRUE)[nearZeroVar(pmltraincnum,\r\n                                              saveMetrics=TRUE)$zeroVar==TRUE,\r\n                                  ]\r\n        )\r\n```\r\n\r\n### Removing correlated predictors\r\n\r\nIn this section I considered to remove those predictors that have high (>0.99) \r\ncorrelation with one ore more other predictors, as they bring redundancy in the\r\nmodel. (Correlation over 0.99 shows very high similarity between variables.)\r\n\r\n```{r}\r\npmltraincnumnonzvnoncorr <- pmltraincnumnonzv[,-findCorrelation(cor(pmltraincnumnonzv[,-1]), cutoff = 0.99)]\r\n```\r\n\r\nThis procedure removed the following 12 variables:\r\n\r\n```{r}\r\ncolnames(pmltraincnumnonzv[,findCorrelation(cor(pmltraincnumnonzv[,-1]), cutoff = 0.99)])\r\n```\r\n\r\n### Removing Near Zero Covariates\r\n\r\nI was still over 100 variables to include my model, so finally, I turned back to \r\nthe previously used `nearZeroVar()` function to identify not only zero \r\ncovariates but also **near** zero covariates.\r\n\r\n```{r}\r\npmlfinaldata <- pmltraincnumnonzvnoncorr[,nearZeroVar(pmltraincnumnonzvnoncorr,saveMetrics = TRUE)$nzv==FALSE]\r\n```\r\n\r\nThis is a longer list of variables that have been flagged as low contributors to\r\nthe total variability:\r\n\r\n```{r}\r\nrownames(\r\n    nearZeroVar(pmltraincnumnonzvnoncorr,\r\n                saveMetrics = TRUE)[nearZeroVar(pmltraincnumnonzvnoncorr,\r\n                                                saveMetrics = TRUE)$nzv==TRUE,\r\n                                    ]\r\n        )\r\n```\r\n\r\nSo I ended up with the following dataset to be analysed:\r\n\r\n```{r}\r\nstr(pmlfinaldata)\r\n```\r\n\r\n# Model Fitting\r\n\r\nFirst, let's train a model with Random Forest, as it is a widely used classifier. (I\r\ncould have used Linear Discriminants, multinomial logit, Support Vector \r\nMachines, etc..)\r\n\r\n```{r cache=TRUE}\r\n# Make use of process parallelization\r\nregisterDoParallel(makeCluster(detectCores()))\r\n\r\n# Train the model\r\nmdl <- train(classe~., \r\n             data=pmlfinaldata, \r\n             method = \"rf\", \r\n             trControl = trainControl(method = \"cv\", number = 5, allowParallel = TRUE)\r\n             )\r\nmdl\r\nmdl$finalModel\r\n```\r\n\r\nThe model accuracy is really good, over 99%. Let's take a look at the confusion\r\nmatrix:\r\n\r\n```{r}\r\nconfusionMatrix(predict(mdl,pmltestcnum),pmltestcnum$classe)\r\n```\r\n\r\nAccording to model fitting results on the test set, I calculated the following\r\nOut Of Sample Error rate of my model:\r\n\r\n```{r}\r\nprediction <- predict(mdl,pmltestcnum)\r\n# Out of sample error percentage:\r\npaste0(round(sum(pmltestcnum$classe != prediction)/length(prediction)*100,2),\"%\")\r\n```\r\n\r\n# Submission\r\n\r\nFor submission I read in the given test set that includes 20 observation.\r\n\r\n```{r}\r\n# Test dataset - for submission\r\norig_pml_test <- read.csv(\"pml-testing.csv\")\r\n```\r\n\r\nI use my previously defined model to predict the requested `classe` variable.\r\n\r\n```{r}\r\nsubm_pred <- predict(mdl,orig_pml_test)\r\nsubm_pred\r\n```\r\n\r\nI defined `pml_write_files` function to write out predictions into 20 separate \r\nfiles. (Source: Practical Machine Learning, Course Project Submission \r\nassignement instructions, \r\nurl=https://class.coursera.org/predmachlearn-034/assignment/view?assignment_id=5)\r\n\r\n```{r}\r\npml_write_files = function(x){\r\n  n = length(x)\r\n  for(i in 1:n){\r\n    filename = paste0(\"problem_id_\",i,\".txt\")\r\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n  }\r\n}\r\npml_write_files(subm_pred)\r\n```\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}